# Final Report
### Emily Martin | Spring 2021 | eem80@pitt.edu

### Introduction
#### Summary
The goal of this project was to analysis newspaper articles from 2015 during the refugee crisis in Germany. Specifically, I wanted to look at several sources with different political leanings and compare their sentiments towards refugees. I scraped articles from four different sources and then used SpaCy, along with their Sentiws pipeline extension, to perform sentiment analysis on them. I also did some analysis of the sentiment analysis method itself, shortcomings and potential improvements, as well as some word-based analysis of the sources.
#### Background/Motivation
The Flüchtlingskrise, or the refugee crisis, refers to the period of time ~2015-2016/17 when hundreds of thousands of refugees from the Middle East and Africa flooded into Europe escaping violence and war. Many came over land, into Turkey and Greece, and many also braved the Mediterranean Sea, into Italy. Because these countries alone could not take all the asylum seekers many EU countries decided to welcome a certain number. Germany in particular took on a huge number of people, a move that was controversial then and continued to spark debates up until the present. In particular a lot of the popularization and the rise of the AfD (Alternative für Deustchland, a very conservative, "Germany for the Germans" kind of party) and PEGIDA (Patriotische Europäer gegen die Islamisierung des Abendlandes/Patriotic Europeans Against the Islamicisation of the Occident) and other far right/nationalistic groups can be traced back to the refugee crisis. The reaction in Germany was not all negative, with a big "Welcome culture" and people volunteering to help in all sorts of ways. I was one of them in 2017 when I taught English at a high school and that was a big part of my motivation for this project.
#### Hypothesis
Because right-wing politics tend to be much more anti-immigration my hypothesis was that more right-leaning newspapers would report on the refugees more negatively than those that leaned more left. Conversely more left-leaning sources would express more positive sentiments towards refugees.

### Process
When I first began this project I did not know anything about web scraping and of the five original sources I chose only one (Der Zeit) supported an API. To my knowledge it is also just the only German newspaper that has one at all. Which leads me to the first big issue, which was copyright. None of these newspapers are keen on anyone sharing any data gathered, not do they make it easy to tell if you can scrape the site at all. One source, FAZ (Frankfürter Allgemeine Zeitung) explicitly forbids web scraping of any kind. This is a shame because they were my middle/right source and so I was left with one very left (Die Tageszeitung), one middle left (Süddeutsche Zeitung), one more central, but still more left-leaning than right-leaning (Der Zeit) and then only one very right source (Junge Freiheit). The uneven distribution is not ideal, but at that point I just needed to get my data. Overall, data gathering took a lot longer than anticipated, up until the second progress report, which did not leave me nearly as much time as I wanted for analysis. Each site is structured differently, so even after learning how to scrape with Urllib and BeautifulSoup it was a process to adapt the code to each site. Later I also ran into the issue that the scraping scripts can be temperamental and in the case of Junge Freiheit the 'date filter' which is supposed to only allow articles from 2015 stopped working because of something on their end that I was not able to determine. Filtering once the data was in the dataframe proved easy enough, but it was still a disheartening discovery. I also had a problem with the Süddeutsche Zeitung script only getting articles for the first three months of 2015 (I think the site limits each search to 100 pages) and I was able to fix this by sorting by relevance instead of date.

The second big issue came with Sentiws, the SpaCy pipeline extension for German sentiment analysis. It assigns certain words weights (-1 to 1) which can then be used to calculate a total sentiment score. However, very few words actually had weights and Flüchtling, my search term, has a negative weight. Not only does that not jibe with both my knowledge of the word, but native German speakers have assured me it is not innately negative - if anything the opposite. Not only was Flüchtling negative, but the majority of my articles were labeled negative as well. While there was no quick fix that worked, I tried removing 'Flüchtling' as well as a more refined labeling scheme, I was able to do a deeper dive into the most common words and their scores and determine some potential explanations. Overall, while there were some stumbling blocks along the way and I learned many things I should have done too late, I think I was able to produce some results and learn a ton along the way!

### Data
- [Der Zeit](https://www.zeit.de/index?utm_referrer=https%3A%2F%2Fwww.google.com), centrist/liberal in its political leanings, was the first site that I scraped and they kindly support and API. Through the API I was able to enter a search term and collect article links, which I was then able to iterate through with urllib and BeautifulSoup to get the article text. Here is a link to the [nbviewer version](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2021/Fluechtlingskrise-Sentiment-Analysis/blob/main/scraping/zeit.ipynb) of my Zeit scraping script. In the end I was able to collect 573 articles.

- [taz.ipynb](https://github.com/Data-Science-for-Linguists-2021/Fluechtlingskrise-Sentiment-Analysis/blob/main/scraping/taz.ipynb), which leans left-wing/green and is the most liberal of my sources was the next site I scraped. This one required me to manually collect links through Pitts archive, which was quite time consuming, and so I was only able to get 100 links. I iterated through them in a similar manner to the Zeit links and was able to populate a dataframe with dates and text, here is a link to the [nbviewer version](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2021/Fluechtlingskrise-Sentiment-Analysis/blob/main/scraping/taz.ipynb). In the end I was able to get 100 articles.

- [Der Süddeutsche Zeitung](https://www.sueddeutsche.de), which leans left-liberal was my next stop, and this one required two scraping steps. First I scraped urls for relevant articles using their base url and a nifty add on url chunk that included: search string/date range/sort by relevance (very kind of them to make it so simple), which I then iterated though to get the article text. Here is a link to the [nbviewer version](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2021/Fluechtlingskrise-Sentiment-Analysis/blob/main/scraping/Süddeutsche_zeitung.ipynb) of the script. In the end I was able to get 990 articles. By far the most for any source!

- [Junge Freiheit](https://jungefreiheit.de), which leans fairly far right, was my last stop. This site did not kindly include a date range in the url so I had to write the first scraping code (that collected to article links) to filter by date. As I mentioned above this stopped working perfectly for reasons I could not determine, however I was still able to sort by date once the dataframe was built. Here is the [nbviewer version](https://nbviewer.jupyter.org/github/Data-Science-for-Linguists-2021/Fluechtlingskrise-Sentiment-Analysis/blob/main/scraping/Junge_Freiheit.ipynb) of the script. In the end I got 76 articles. The fewest of any source, but they are a fairly small weekly newspaper.

### Analysis
I began my [first analysis](#Sentiment-Analysis) by building a SpaCy pipeline including Sentiws which I then passed all the text through, having it collect the sentiment scores when there was one. I then took the mean of the scores for each article and ran that through a custom function to give each article a 'pos' or 'neg' label.
Here is a plot of that first run!
![png](images/sentiment_bysource1.png)

### conclusions/findings
